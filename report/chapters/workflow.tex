\section{Workflow}
\subsection{Setup}
\paragraph{Hardware}
Models were being trained in Google Colab using Google TPU v2 \footnote{\url{https://cloud.google.com/tpu}}. It has 64 GB HBM memory and a compute capability of 180 teraflops. In our experiments, TPUs outperform NVidia GPUs available in Colab by running six time faster.
\paragraph{Software}
In order to train transformer models, we used Tensorflow with Keras \cite{keras_io} and the Huggingface \cite{huggingface_co} python library to retrieve pre-trained models.

\subsection{Dataset management}
As a guideline, we trained our models using the entire anki dataset and 20 percent of the Europarl bilingual dataset. In order to train a model for machine translation, the sentences inside the datasets that are written in natural language have to be preprocessed. This phase is called tokenization. Every encoder has its own tokenizer, that is a module that parse words into tokens, that are integers essentially. These two datasets don't fit together into the RAM available in Google Colab, so it was necessary to split them into smaller one in order to apply the tokenizer. After that every tokenization on the small piece of the dataset was completed, we merge the results using Tensorflow Dataset API, in order to build the entire tokenized dataset. This is a mandatory step that allow to train any model using very large dataset. In fact, Tensorflow Dataset swaps data into the secondary memory if the main memory in not enough to contain the entire data. At training time, Tensorflow Dataset loads data into the accelerator (GPU or TPU) memory in batches of equal dimension. 

\subsection{Tokenization}
As announced earlier, any dataset made up of sentences written in natural language has to be tokenized before its usage. The Huggingface library provides a Tokenizer class that can retrieve a pre-trained tokenizer from a wide database and tokenize any sentence you provide. As shown in the example below in code \ref{lst:bert_tokenizer}, we create a dataset with a fixed length of tokens, padded with zeros whether the sentence was shorter than the maximum length or truncated in case it exceed the pre-defined size.
\begin{listing}[H]
\begin{minted}[fontsize=\small, linenos, breaklines]{python}
tok_trg = "dbmdz/bert-base-italian-cased"
tokenizer_target = BertTokenizerFast.from_pretrained(tok_trg) 
tokens_source = tokenizer_source(source_set, truncation=True,                      padding="max_length", return_tensors="tf",                         max_length=sequence_length).data["input_ids"]
\end{minted}
\caption{Example of code to tokenize a piece of dataset using Huggingface Bert Tokenizer.}
\label{lst:bert_tokenizer}
\end{listing}