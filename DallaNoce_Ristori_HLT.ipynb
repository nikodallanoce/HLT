{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DallaNoce_Ristori_HLT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZYveYZaAJ4CK",
        "orp8P-ImLWro",
        "sGX5wZEjNSnO",
        "nQ9zRkqAPpDr",
        "hzjwWWkWQQW0",
        "7E-8waaOQ9_x",
        "5lSeCXnFRLUo",
        "ZeUIc8CARPLg",
        "btGbS2DvTRzS",
        "jh6qz5N7Tf3v",
        "sJ52WxcYf75B"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5gTl7VXJiwo"
      },
      "source": [
        "**Human Language Technologies Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyUEYbqJqPs"
      },
      "source": [
        "**Authors:** Dalla Noce Niko, Ristori Alessandro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwktbIvfJJ71"
      },
      "source": [
        "#HLT Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsAoE5E4SUuh"
      },
      "source": [
        "This work is higly based on the tensorflow tutorial https://www.tensorflow.org/text/tutorials/transformer, our aim was to introduce BERT as an encoder in the model and try combinations with different architectures (both RNNs and transformers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYveYZaAJ4CK"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0xEHybmMaM3"
      },
      "source": [
        "We need to install the transformers package to use the models and tokenizers from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE9UOT6MJG_7",
        "outputId": "9ac00742-a788-42e2-a0fc-9a3360ccbcfd"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8LIQXIKSc4"
      },
      "source": [
        "Import the libraries needed for the project to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTS92bWiKIQx"
      },
      "source": [
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOBsbNdFMrNh"
      },
      "source": [
        "The model training is going to run on TPUs since they are the optimized for working with tensors, to do so we need colab to assign us as much TPUs as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWGEpMKhKiko",
        "outputId": "f25d1a17-a66d-4049-a6af-6e6723e32a33"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.121.42:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.121.42:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSN8UPU5LJ2m"
      },
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orp8P-ImLWro"
      },
      "source": [
        "##Preprocess the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC2VjPj2LzhY"
      },
      "source": [
        "Let's define the method to preprocess the anki dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfG1lSnbLZyl"
      },
      "source": [
        "def create_dataset_anki(name: str, preprocessed:bool) -> (list, list):\n",
        "    with open(name, encoding=\"UTF-8\") as datafile:\n",
        "        src_set = list()\n",
        "        dst_set = list()\n",
        "        for sentence in datafile:\n",
        "            sentence = sentence.split(\"\\t\")\n",
        "            src_set.append(sentence[0])\n",
        "            if preprocessed:\n",
        "                dst_set.append(sentence[1].split(\"\\n\")[0])\n",
        "            else:\n",
        "                dst_set.append(sentence[1])\n",
        "\n",
        "    return src_set, dst_set"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgPVmTY7L6Tf"
      },
      "source": [
        "We assume that the dataset was uploaded on colab with a zip file, we need to extract it and then we can build our lists using the previous method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gamUp8XiLrwB",
        "outputId": "20b8fb1a-3386-453c-d1bf-ac29e3dfb040"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"dataset_anki_it.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "\n",
        "en_set, it_set = create_dataset_anki(\"ita_preprocessed.txt\", True)\n",
        "print(\"Il corpus ha dimensione: {0}\".format(len(en_set)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il corpus ha dimensione: 352040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGX5wZEjNSnO"
      },
      "source": [
        "##Build the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUn2kITYNVa3"
      },
      "source": [
        "Before we create the dataset from our lists, we have to tokenize each sentence from the corpus by using the BERT tokenizer for english and the one for italian. Moreover we can get the number of tokens for both source and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7r7wQatNVDs"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Create the tokenizers and get the number of tokens\n",
        "tokenizer_en = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer_it = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-uncased\")\n",
        "v_size_en = tokenizer_en.vocab_size\n",
        "v_size_it = tokenizer_it.vocab_size"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdfUZv0YOL1I"
      },
      "source": [
        "# Tokenize the dataset\n",
        "tokens_en = tokenizer_en(en_set, add_special_tokens=True,\n",
        "                          truncation=True, padding=\"max_length\", return_attention_mask=True,\n",
        "                          return_tensors=\"tf\", max_length=30).data[\"input_ids\"]\n",
        "tokens_it = tokenizer_it(it_set, add_special_tokens=True,\n",
        "                          truncation=True, padding=\"max_length\", return_attention_mask=True,\n",
        "                          return_tensors=\"tf\", max_length=30).data[\"input_ids\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dwetveOqdO"
      },
      "source": [
        "Then we build the tf dataset and we split it in training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZDQBwuPOxEU"
      },
      "source": [
        "def split_set(dataset: tf.data.Dataset,\n",
        "              tr: float = 0.8,\n",
        "              val: float = 0.1,\n",
        "              ts: float = 0.1,\n",
        "              shuffle: bool = True) -> (tf.data.Dataset, tf.data.Dataset, tf.data.Dataset):\n",
        "    if tr+val+ts != 1:\n",
        "        raise ValueError(\"Train, validation and test partition not allowed with such splits\")\n",
        "\n",
        "    dataset_size = dataset.cardinality().numpy()\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(dataset_size)\n",
        "\n",
        "    tr_size = int(tr * dataset_size)\n",
        "    val_size = int(val * dataset_size)\n",
        "\n",
        "    tr_set = dataset.take(tr_size)\n",
        "    val_set = dataset.skip(tr_size).take(val_size)\n",
        "    ts_set = dataset.skip(tr_size).skip(val_size)\n",
        "    return tr_set, val_set, ts_set"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc8054ACO3Zh",
        "outputId": "7b93df22-85ef-4771-a02b-e6f067857fce"
      },
      "source": [
        "# Build the dataset and split it in train, validation and test\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tokens_en, tokens_it))  # build the tf dataset\n",
        "tr_set, val_set, ts_set = split_set(dataset, 0.8, 0.1, 0.1)  # split the tf dataset\n",
        "print(\"Dimensione training set: {0}\".format(len(tr_set)))\n",
        "print(\"Dimensione validation set: {0}\".format(len(val_set)))\n",
        "print(\"Dimensione test set: {0}\".format(len(ts_set)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensione training set: 281632\n",
            "Dimensione validation set: 35204\n",
            "Dimensione test set: 35204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWU92FjKO9J3"
      },
      "source": [
        "After we have built our development and test set, we need to split the first one (both training and validation) in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOiUgmSVPK1H"
      },
      "source": [
        "def make_batches(dataset_src_dst: tf.data.Dataset, batch_size: int):\n",
        "    return dataset_src_dst.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hkK5H8iPNqj"
      },
      "source": [
        "with strategy.scope():\n",
        "  tr_batches = make_batches(tr_set, 128)\n",
        "  val_batches = make_batches(val_set, 128)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ9zRkqAPpDr"
      },
      "source": [
        "##Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfx4FqnxP8-H"
      },
      "source": [
        "Attention layers see their input as a set of vectors, with no sequential order. This model also doesn't contain any recurrent or convolutional layers. Because of this a \"positional encoding\" is added to give the model some information about the relative position of the tokens in the sentence.\n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of tokens in a sentence. So after adding the positional encoding, tokens will be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ-kiJzePpzt"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzjwWWkWQQW0"
      },
      "source": [
        "##Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LsUdToKQXGa"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLNVe-9SQSo3"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32) #cambiato tf.math.equal in tf.math.not_equal\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVaNZkusQaN8"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvFbrSHLQdcr"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E-8waaOQ9_x"
      },
      "source": [
        "##Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lSeCXnFRLUo"
      },
      "source": [
        "###Self-attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmLl3gU7RAX6"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeUIc8CARPLg"
      },
      "source": [
        "###Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3gKqb57RSNU"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k=None, q=None, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(\n",
        "            scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3K1uHFRyW9"
      },
      "source": [
        "##Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQWBUtfjkPMl"
      },
      "source": [
        "As we know from the HLT course, NMT models are based on the encoder-decoder paradigm, therefore we have to build both. We based the architecture of those layers from the paper \"Attention is all you need\" from Vaswani et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZdCgcJpR11c"
      },
      "source": [
        "###Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43KG2XY8kiOT"
      },
      "source": [
        "The single layer of the encoder transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWOzO7bDR8dU"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 layers_size: int,\n",
        "                 num_heads: int,\n",
        "                 dff: int,\n",
        "                 dropout: float = 0.1) -> None:\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # self.mha = MultiHeadAttention(layers_size, num_heads)\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads, layers_size)\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "              tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "              tf.keras.layers.Dense(layers_size)  # (batch_size, seq_len, d_model)\n",
        "            ])\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, src_tokens: tf.Tensor, training: bool, mask: tf.Tensor) -> tf.Tensor:\n",
        "        # attn_output, _ = self.mha.call(src_tokens, src_tokens,\n",
        "        #                               src_tokens, mask)  # (batch_size, input_seq_len, layers_size)\n",
        "\n",
        "        attn_output = self.mha(src_tokens, src_tokens,\n",
        "                                       src_tokens, mask)  # (batch_size, input_seq_len, layers_size)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(src_tokens + attn_output)  # (batch_size, input_seq_len, layers_size)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, layers_size)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, layers_size)\n",
        "        return out2"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVU-JlJ6SB9x"
      },
      "source": [
        "class EncoderTransformer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, num_layers: int,\n",
        "                 layers_size: int,\n",
        "                 num_heads: int,\n",
        "                 dff: int,\n",
        "                 src_vocab_size: int,\n",
        "                 maximum_position_encoding: int,\n",
        "                 dropout: float = 0.1) -> None:\n",
        "        super(EncoderTransformer, self).__init__()\n",
        "\n",
        "        self.layers_size = layers_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(src_vocab_size, layers_size)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.layers_size)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(layers_size, num_heads, dff, dropout) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, src_tokens: tf.Tensor, training: bool, mask: tf.Tensor) -> tf.Tensor:\n",
        "        seq_len = tf.shape(src_tokens)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(src_tokens)  # (batch_size, input_seq_len, layers_size)\n",
        "        x *= tf.math.sqrt(tf.cast(self.layers_size, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i].call(x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, layers_size)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk5x1zELSFyl"
      },
      "source": [
        "###Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WwlG430kpb7"
      },
      "source": [
        "The single layer of the decoder transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVmDmBHsSLHo"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 layers_size: int,\n",
        "                 num_heads: int,\n",
        "                 dff: int,\n",
        "                 dropout=0.1) -> None:\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # self.mha1 = MultiHeadAttention(layers_size, num_heads)\n",
        "        # self.mha2 = MultiHeadAttention(layers_size, num_heads)\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads, layers_size)\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads, layers_size)\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "              tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "              tf.keras.layers.Dense(layers_size)  # (batch_size, seq_len, d_model)\n",
        "            ])\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    @tf.function()\n",
        "    def call(self,\n",
        "             dst_tokens: tf.Tensor,\n",
        "             enc_output: tf.Tensor,\n",
        "             training: bool,\n",
        "             look_ahead_mask: tf.Tensor,\n",
        "             padding_mask: tf.Tensor):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, layers_size)\n",
        "\n",
        "        # attn1, attn_weights_block1 = self.mha1(dst_tokens, dst_tokens, dst_tokens,\n",
        "        #                                       look_ahead_mask)  # (batch_size, target_seq_len, layers_size)\n",
        "        out_att1 = self.mha1(dst_tokens, dst_tokens, dst_tokens,\n",
        "                             look_ahead_mask)  # (batch_size, target_seq_len, layers_size)\n",
        "        attn1 = self.dropout1(out_att1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + dst_tokens)\n",
        "\n",
        "        # attn2, attn_weights_block2 = self.mha2(out1, enc_output,\n",
        "        #                                       enc_output, padding_mask)  # (batch_size, target_seq_len, layers_size)\n",
        "        out_att2 = self.mha2(out1, enc_output, enc_output,\n",
        "                             padding_mask)  # (batch_size, target_seq_len, layers_size)\n",
        "        attn2 = self.dropout2(out_att2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, layers_size)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, layers_size)\n",
        "\n",
        "        # return out3, attn_weights_block1, attn_weights_block2\n",
        "        return out3, # out_att1[1] , out_att2[1]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFQJbb3nSwhV"
      },
      "source": [
        "class DecoderTransformer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 layers_size: int,\n",
        "                 num_heads: int,\n",
        "                 dff: int,\n",
        "                 target_vocab_size: int,\n",
        "                 maximum_position_encoding: int, dropout=0.1) -> None:\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.layers_size = layers_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, layers_size)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, layers_size)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(layers_size, num_heads, dff, dropout) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self,\n",
        "             dst_tokens: tf.Tensor,\n",
        "             enc_output: tf.Tensor,\n",
        "             training: bool,\n",
        "             look_ahead_mask: tf.Tensor,\n",
        "             padding_mask: tf.Tensor) -> (tf.Tensor, tf.Tensor):\n",
        "        seq_len = tf.shape(dst_tokens)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(dst_tokens)  # (batch_size, target_seq_len, layers_size)\n",
        "        x *= tf.math.sqrt(tf.cast(self.layers_size, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            x = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            # attention_weights[f'decoder_layer{i + 1}_block1'] = block1\n",
        "            # attention_weights[f'decoder_layer{i + 1}_block2'] = block2\n",
        "\n",
        "        return x #, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nOatGOPS12t"
      },
      "source": [
        "##Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz517yyZS7cO"
      },
      "source": [
        "The model consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMc72ufbS_bh"
      },
      "source": [
        "class TransformerNMT(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder: tf.keras.layers.Layer,\n",
        "                 decoder: tf.keras.layers.Layer,\n",
        "                 dst_v_size: int,\n",
        "                 lan_src: str = \"english\",\n",
        "                 lan_dst: str = \"italian\") -> None:\n",
        "        super(TransformerNMT, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.lan_src = lan_src\n",
        "        self.lan_dst = lan_dst\n",
        "        self.final_layer = tf.keras.layers.Dense(dst_v_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def __create_masks(src: tf.Tensor, dst: tf.Tensor) -> (tf.Tensor, tf.Tensor):\n",
        "        # Encoder padding mask\n",
        "        enc_padding_mask = create_padding_mask(src)\n",
        "\n",
        "        # Used in the 2nd attention block in the decoder.\n",
        "        # This padding mask is used to mask the encoder outputs.\n",
        "        dec_padding_mask = create_padding_mask(src)\n",
        "\n",
        "        # Used in the 1st attention block in the decoder.\n",
        "        # It is used to pad and mask future tokens in the input received by\n",
        "        # the decoder.\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(dst)[1])\n",
        "        dec_target_padding_mask = create_padding_mask(dst)\n",
        "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "        return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "\n",
        "    def call(self, inputs: list, training: bool) -> (tf.Tensor, tf.Tensor):\n",
        "        # Keras models prefer if you pass all your inputs in the first argument\n",
        "        src, dst = inputs\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.__create_masks(src, dst)\n",
        "\n",
        "        enc_output = self.encoder(src, training, enc_padding_mask)  # (batch_size, inp_seq_len, layers_size)\n",
        "\n",
        "        # dec_output, attention_weights = self.decoder(dst, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        dec_output = self.decoder(dst, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btGbS2DvTRzS"
      },
      "source": [
        "##Optmizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "busHjsItTWA5"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def get_config(self):\n",
        "        pass\n",
        "\n",
        "    def __init__(self, layers_size, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.layers_size = layers_size\n",
        "        self.layers_size = tf.cast(self.layers_size, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.layers_size) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh6qz5N7Tf3v"
      },
      "source": [
        "##Loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ua0qSrATw-N"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2, output_type=tf.int32))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl1cLbXBT3s0"
      },
      "source": [
        "with strategy.scope():\n",
        "  train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "  train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGO3ZxwhUIL1"
      },
      "source": [
        "##Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZhNk1fSUKJ9"
      },
      "source": [
        "with strategy.scope():\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, transformer: TransformerNMT):\n",
        "        learning_rate = CustomSchedule(transformer.decoder.layers_size)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "        self.transformer = transformer\n",
        "\n",
        "    @tf.function()\n",
        "    def __train_step(self, src: tf.Tensor, dst: tf.Tensor) -> None:\n",
        "        dst_inp = dst[:, :-1]\n",
        "        dst_real = dst[:, 1:]\n",
        "\n",
        "        def step_fn(dst_inp, dst_real):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # predictions, _ = self.transformer([src, dst_inp], training=True)\n",
        "                predictions = self.transformer([src, dst_inp], training=True)\n",
        "                loss = loss_function(dst_real, predictions)\n",
        "\n",
        "            gradients = tape.gradient(loss, self.transformer.trainable_variables)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.transformer.trainable_variables))\n",
        "\n",
        "            train_loss(loss)\n",
        "            train_accuracy(accuracy_function(dst_real, predictions))\n",
        "\n",
        "        strategy.run(step_fn, args=(dst_inp, dst_real))\n",
        "\n",
        "    def train(self, epochs: int, tr_batches) -> None:\n",
        "        for epoch in range(epochs):\n",
        "            start = time.time()\n",
        "\n",
        "            train_loss.reset_states()\n",
        "            train_accuracy.reset_states()\n",
        "\n",
        "            for (batch, (src, dst)) in enumerate(tr_batches):\n",
        "                self.__train_step(src, dst)\n",
        "\n",
        "                if batch % 50 == 0:\n",
        "                    print(\n",
        "                        f'Epoch {epoch + 1} Batch {batch} '\n",
        "                        f'Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                pass\n",
        "                # print(\"save\")\n",
        "\n",
        "            print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "            print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccFwVc9HU70h"
      },
      "source": [
        "##Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "VJ3VnYtqU9KQ",
        "outputId": "056657b6-135a-4566-c477-2d871d9182d0"
      },
      "source": [
        "# Setup the hyperparameters\n",
        "num_layers = 6\n",
        "layers_size = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "\n",
        "# Build the model\n",
        "encoder = EncoderTransformer(num_layers, layers_size, num_heads, dff, v_size_en, 30)\n",
        "decoder = DecoderTransformer(num_layers, layers_size, num_heads, dff, v_size_it, 30)\n",
        "model = TransformerNMT(encoder, decoder, v_size_it)\n",
        "\n",
        "# Build the trainer and train the model\n",
        "trainer = Trainer(model)\n",
        "trainer.train(3, tr_batches)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8d1c3be0cba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Build the trainer and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-950d843489d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, tr_batches)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3988\u001b[0m     \u001b[0;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3989\u001b[0m     \u001b[0;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3990\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3991\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-24-950d843489d8>:21 step_fn  *\n        predictions = self.transformer([src, dst_inp], training=True)\n    <ipython-input-19-e840d48cc258>:38 call  *\n        x = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n    <ipython-input-26-e1b853f5eb22>:38 call  *\n        out_att1 = self.mha1(dst_tokens, dst_tokens, dst_tokens,\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py:488 call\n        self._build_from_signature(query=query, value=value, key=key)\n    /usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py:313 _build_from_signature\n        self._query_shape = tf.TensorShape(query)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:765 __init__\n        self._dims = [Dimension(d) for d in dims]\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:765 <listcomp>\n        self._dims = [Dimension(d) for d in dims]\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:209 __init__\n        .format(value, type(value))), None)\n    <string>:3 raise_from\n        \n\n    TypeError: Dimension value must be integer or None or have an __index__ method, got value '<tf.Tensor 'dst_tokens:0' shape=(128, 29, 512) dtype=float32>' with type '<class 'tensorflow.python.framework.ops.Tensor'>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjIo3f2ubjFd"
      },
      "source": [
        "model.save_weights(\"nmt_transformer_transformer.h5\")\n",
        "\n",
        "transformer_model = TransformerNMT(encoder, decoder, v_size_it)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = transformer_model([temp_input, temp_target], training=False)\n",
        "\n",
        "transformer_model.load_weights(\"nmt_transfomer_transformer.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ52WxcYf75B"
      },
      "source": [
        "##Translator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miFhmV9Kf9oV"
      },
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, src_tokenizer: BertTokenizer, targ_tokenizer: BertTokenizer, transformer: TransformerNMT) -> None:\n",
        "        super(Translator, self).__init__()\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.targ_tokenizer = targ_tokenizer\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=30) -> str:\n",
        "        sentence_tok = self.src_tokenizer(sentence, padding='max_length', return_tensors='tf', max_length=max_length,\n",
        "                                          add_special_tokens=True).data['input_ids']\n",
        "\n",
        "        encoder_input = tf.reshape(sentence_tok, [1, max_length])\n",
        "\n",
        "        start_end = self.targ_tokenizer(\"\", padding='max_length', return_tensors='np',\n",
        "                                        max_length=3, add_special_tokens=True).data['input_ids']\n",
        "\n",
        "        start = tf.convert_to_tensor([start_end[0, 0]], dtype=tf.int32)\n",
        "        end = tf.convert_to_tensor([start_end[0, 1]], dtype=tf.int32)\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a python list) so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "            predictions, _ = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "            # select the last token from the seq_len dimension\n",
        "            predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32)\n",
        "\n",
        "            # concatentate the predicted_id to the output which is given to the decoder\n",
        "            # as its input.\n",
        "            output_array = output_array.write(i + 1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        output = output.numpy().tolist()[0]\n",
        "        # out = list()\n",
        "        # for r in output:\n",
        "        #    for e in r:\n",
        "        #        out.append(e)\n",
        "        \n",
        "        # text = self.targ_tokenizer.convert_ids_to_tokens(out)\n",
        "        text = self.targ_tokenizer.convert_ids_to_tokens(output)\n",
        "\n",
        "        return self.targ_tokenizer.convert_tokens_to_string(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMNTo2qZg-he"
      },
      "source": [
        "Let's try the translator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5QA2oyohAeM"
      },
      "source": [
        "# Build the translator\n",
        "translator = Translator(tokenizer_en, tokenizer_it, model)\n",
        "\n",
        "# Translate some examples\n",
        "out = translator(\"I want to beat you hard.\")\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1eBFn5nksru"
      },
      "source": [
        "Da qui in poi va messo BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECVK79hqpBAz"
      },
      "source": [
        "##BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm_8cYYUpD-x"
      },
      "source": [
        " # Setup the hyperparameters\n",
        "num_layers = 6\n",
        "layers_size = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "\n",
        "from transformers import TFBertModel\n",
        "\n",
        "encoderBert: TFBertModel = TFBertModel.from_pretrained(\"bert-base-uncased\", trainable=False)\n",
        "decoder = DecoderTransformer(num_layers, layers_size, num_heads, dff, v_size_it, 30)\n",
        "model_bert = TransformerNMT(encoderBert, decoder, v_size_it)\n",
        "\n",
        "trainer = Trainer(model_bert)\n",
        "# out = encoderBert(tokens_en)\n",
        "out\n",
        "# trainer.train(5, tr_batches)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}