{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "DallaNoce_Ristori_HLT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZYveYZaAJ4CK",
        "orp8P-ImLWro",
        "sGX5wZEjNSnO",
        "eNH_DBR_Fwn0",
        "YX3K1uHFRyW9"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5gTl7VXJiwo"
      },
      "source": [
        "**Human Language Technologies Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyUEYbqJqPs"
      },
      "source": [
        "**Authors:** Dalla Noce Niko, Ristori Alessandro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwktbIvfJJ71"
      },
      "source": [
        "#HLT Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsAoE5E4SUuh"
      },
      "source": [
        "This work is higly based on the tensorflow tutorial https://www.tensorflow.org/text/tutorials/transformer, our aim was to introduce BERT as an encoder in the model and try combinations with different architectures (both RNNs and transformers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYveYZaAJ4CK"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0xEHybmMaM3"
      },
      "source": [
        "We need to install the transformers package to use the models and tokenizers from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE9UOT6MJG_7",
        "outputId": "2f9ef3b9-1f29-47e6-9548-bb14bd6eec01"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.0-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8LIQXIKSc4"
      },
      "source": [
        "Import the libraries needed for the project to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTS92bWiKIQx"
      },
      "source": [
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOBsbNdFMrNh"
      },
      "source": [
        "The model training is going to run on TPUs since they are the optimized for working with tensors, to do so we need colab to assign us as much TPUs as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWGEpMKhKiko",
        "outputId": "d4b4e41e-fdf7-43d8-97ae-3e37f1fceef5"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.53.152.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.53.152.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSN8UPU5LJ2m"
      },
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orp8P-ImLWro"
      },
      "source": [
        "##Preprocess the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC2VjPj2LzhY"
      },
      "source": [
        "Let's define the method to preprocess the anki dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfG1lSnbLZyl"
      },
      "source": [
        "def create_dataset_anki(name: str, preprocessed:bool) -> (list, list):\n",
        "    with open(name, encoding=\"UTF-8\") as datafile:\n",
        "        src_set = list()\n",
        "        dst_set = list()\n",
        "        for sentence in datafile:\n",
        "            sentence = sentence.split(\"\\t\")\n",
        "            src_set.append(sentence[0])\n",
        "            if preprocessed:\n",
        "                dst_set.append(sentence[1].split(\"\\n\")[0])\n",
        "            else:\n",
        "                dst_set.append(sentence[1])\n",
        "\n",
        "    return src_set, dst_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgPVmTY7L6Tf"
      },
      "source": [
        "We assume that the dataset was uploaded on colab with a zip file, we need to extract it and then we can build our lists using the previous method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gamUp8XiLrwB",
        "outputId": "e2bb29d0-b68c-45b3-cff5-f8b2ce540717"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"dataset_anki_it.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "\n",
        "en_set, it_set = create_dataset_anki(\"ita_preprocessed.txt\", True)\n",
        "print(\"The corpus' size is: {0}\".format(len(en_set)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The corpus' size is: 352040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGX5wZEjNSnO"
      },
      "source": [
        "##Build the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUn2kITYNVa3"
      },
      "source": [
        "Before we create the dataset from our lists, we have to tokenize each sentence from the corpus by using the BERT tokenizer for english and the one for italian. Moreover we can get the number of tokens for both source and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7r7wQatNVDs",
        "outputId": "abd26a6d-e278-4e6d-e0e3-c56166d724e6"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)  # suppress warning for transformers\n",
        "\n",
        "# Create the tokenizers and get the number of tokens\n",
        "tokenizer_en = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer_it = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n",
        "v_size_en = tokenizer_en.vocab_size\n",
        "v_size_it = tokenizer_it.vocab_size\n",
        "\n",
        "print(\"Number of tokens for the english dataset: {0}\".format(v_size_en))\n",
        "print(\"Number of tokens for the italian dataset: {0}\".format(v_size_it))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens for the english dataset: 30522\n",
            "Number of tokens for the italian dataset: 31102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unwl54h9PSxV"
      },
      "source": [
        "Let's calculate the max number of tokens allowed, this number is taken such that 99% of the sentences in the dataset are fully tokenized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOTXTbSAIR_O",
        "outputId": "4513aada-c26a-4cc7-f39d-ec7ca4d2bc1e"
      },
      "source": [
        "def set_max_tokens(dataset: list(), language: str = \"en\") -> int:\n",
        "    len_sentences = [len(sentence.split()) for sentence in dataset[:351000]]\n",
        "    # plt.boxplot(len_sentences)\n",
        "    mean_len_sentences = np.mean(len_sentences)\n",
        "    print(\"{0} dataset average sentence length: {1}\".format(language, mean_len_sentences))\n",
        "    max_length = int(mean_len_sentences + 3 * np.std(len_sentences))\n",
        "    print(\"{0} dataset max length allowed: {1}\".format(language, max_length))\n",
        "    return max_length\n",
        "\n",
        "max_length_en = set_max_tokens(en_set, \"en\")\n",
        "max_length_it = set_max_tokens(it_set, \"it\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en dataset average sentence length: 5.492988603988604\n",
            "en dataset max length allowed: 11\n",
            "it dataset average sentence length: 5.35585754985755\n",
            "it dataset max length allowed: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0iDT4uDRdIA"
      },
      "source": [
        "Tokenize the source and target dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdfUZv0YOL1I"
      },
      "source": [
        "# Tokenize the dataset\n",
        "tokens_en = tokenizer_en(en_set[:351000], add_special_tokens=True,\n",
        "                          truncation=True, padding=\"max_length\", return_attention_mask=True,\n",
        "                          return_tensors=\"tf\", max_length=max_length_en).data[\"input_ids\"]\n",
        "tokens_it = tokenizer_it(it_set[:351000], add_special_tokens=True,\n",
        "                          truncation=True, padding=\"max_length\", return_attention_mask=True,\n",
        "                          return_tensors=\"tf\", max_length=max_length_it+1).data[\"input_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t24nGn3-PhfB",
        "outputId": "0f06a9e7-500b-42a1-c584-d51abe3e4218"
      },
      "source": [
        "for _ in range(5):\n",
        "  i = np.random.randint(len(tokens_en))\n",
        "  print(\"En sentence: {0}\\nTokenized sentence: {1}\".format(en_set[i], tokens_en[i]))\n",
        "  print(\"It sentence: {0}\\nTokenized sentence: {1}\\n\".format(it_set[i], tokens_it[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En sentence: Have you lost the receipt?\n",
            "Tokenized sentence: [  101  2031  2017  2439  1996 24306  1029   102     0     0     0]\n",
            "It sentence: Hai perso la ricevuta?\n",
            "Tokenized sentence: [  102   537  3162   143 15721  2937   103     0     0     0     0     0]\n",
            "\n",
            "En sentence: Tom could speak French.\n",
            "Tokenized sentence: [ 101 3419 2071 3713 2413 1012  102    0    0    0    0]\n",
            "It sentence: Tom riusciva a parlare il francese.\n",
            "Tokenized sentence: [  102  5247 19354   111  1691   152  2194   687   103     0     0     0]\n",
            "\n",
            "En sentence: You've got to come tomorrow.\n",
            "Tokenized sentence: [ 101 2017 1005 2310 2288 2000 2272 4826 1012  102    0]\n",
            "It sentence: Dovete venire domani.\n",
            "Tokenized sentence: [ 102 5570 2698 3156  687  103    0    0    0    0    0    0]\n",
            "\n",
            "En sentence: How far is it from here to your house?\n",
            "Tokenized sentence: [ 101 2129 2521 2003 2009 2013 2182 2000 2115 2160  102]\n",
            "It sentence: Quanto dista da qui a casa tua?\n",
            "Tokenized sentence: [ 102  677 5777  165  495  111  695  878 2937  103    0    0]\n",
            "\n",
            "En sentence: The weather forecast is not necessarily reliable.\n",
            "Tokenized sentence: [  101  1996  4633 19939  2003  2025  9352 10539  1012   102     0]\n",
            "It sentence: Le previsioni del tempo non sono necessariamente affidabili.\n",
            "Tokenized sentence: [  102   185  8962   137   647   176   259 10412 17019   687   103     0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dwetveOqdO"
      },
      "source": [
        "Then we build the tf dataset and split it into training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZDQBwuPOxEU"
      },
      "source": [
        "def split_set(dataset: tf.data.Dataset,\n",
        "              tr: float = 0.8,\n",
        "              val: float = 0.1,\n",
        "              ts: float = 0.1,\n",
        "              shuffle: bool = True) -> (tf.data.Dataset, tf.data.Dataset, tf.data.Dataset):\n",
        "    if tr+val+ts != 1:\n",
        "        raise ValueError(\"Train, validation and test partition not allowed with such splits\")\n",
        "\n",
        "    dataset_size = dataset.cardinality().numpy()\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(dataset_size)\n",
        "\n",
        "    tr_size = int(tr * dataset_size)\n",
        "    val_size = int(val * dataset_size)\n",
        "\n",
        "    tr_set = dataset.take(tr_size)\n",
        "    val_set = dataset.skip(tr_size).take(val_size)\n",
        "    ts_set = dataset.skip(tr_size).skip(val_size)\n",
        "    return tr_set, val_set, ts_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc8054ACO3Zh",
        "outputId": "138ab9dc-8a4c-414c-e362-3ec127bbc115"
      },
      "source": [
        "# Build the dataset and split it in train, validation and test\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tokens_en, tokens_it))  # build the tf dataset\n",
        "tr_set, val_set, ts_set = split_set(dataset, 0.8, 0.1, 0.1)  # split the tf dataset\n",
        "print(\"Training set size: {0}\".format(len(tr_set)))\n",
        "print(\"Validation set size: {0}\".format(len(val_set)))\n",
        "print(\"Test set size: {0}\".format(len(ts_set)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 280800\n",
            "Validation set size: 35100\n",
            "Test set size: 35100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWU92FjKO9J3"
      },
      "source": [
        "After we have built our development and test set, we need to split the first one (both training and validation) in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOiUgmSVPK1H"
      },
      "source": [
        "def __format_dataset(eng, ita):\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ita[:, :-1],}, ita[:, 1:])\n",
        "\n",
        "def make_batches(dataset_src_dst: tf.data.Dataset, batch_size: int) -> tf.data.Dataset:\n",
        "    dataset = dataset_src_dst.batch(batch_size)\n",
        "    dataset = dataset.map(__format_dataset)\n",
        "    return dataset.prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
        "\n",
        "# def make_batches(dataset_src_dst: tf.data.Dataset, batch_size: int):\n",
        "#    return dataset_src_dst.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hkK5H8iPNqj"
      },
      "source": [
        "with strategy.scope():\n",
        "  tr_batches = make_batches(tr_set, 128)\n",
        "  val_batches = make_batches(val_set, 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiDUbIoFwajW",
        "outputId": "8a744851-7390-43c4-d0aa-1aeeee39b000"
      },
      "source": [
        "for src, dst in tr_batches.take(1):\n",
        "  print(\"encoder inputs shape: {0}\".format(src[\"encoder_inputs\"].shape))\n",
        "  print(\"decoder inputs shape: {0}\".format(src[\"decoder_inputs\"].shape))\n",
        "  print(\"targets shape: {0}\".format(dst.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder inputs shape: (128, 11)\n",
            "decoder inputs shape: (128, 11)\n",
            "targets shape: (128, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNH_DBR_Fwn0"
      },
      "source": [
        "##Positional embeddings layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfdbcMIgF4IU"
      },
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3K1uHFRyW9"
      },
      "source": [
        "##Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQWBUtfjkPMl"
      },
      "source": [
        "As we know from the HLT course, NMT models are based on the encoder-decoder paradigm, therefore we have to build both. We based the architecture of those layers from the paper \"Attention is all you need\" from Vaswani et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZdCgcJpR11c"
      },
      "source": [
        "###Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43KG2XY8kiOT"
      },
      "source": [
        "The single layer of the encoder transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWOzO7bDR8dU"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, layers_size: int, dense_size: int, num_heads: int, **kwargs) -> None:\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        \n",
        "        self.layers_size = layers_size\n",
        "        self.dense_size = dense_size\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(num_heads, layers_size)\n",
        "        self.dense_proj = tf.keras.Sequential(\n",
        "            [layers.Dense(dense_size, activation=\"relu\"), layers.Dense(layers_size)]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, mask=None) -> tf.Tensor:\n",
        "        if mask is not None:  \n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        else:\n",
        "            print(\"Mask not built\")\n",
        "            assert False\n",
        "        \n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVU-JlJ6SB9x"
      },
      "source": [
        "class EncoderTransformer(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 layers_size: int,\n",
        "                 dense_size: int,\n",
        "                 num_heads: int,\n",
        "                 dropout: float = 0.1) -> None:\n",
        "        super(EncoderTransformer, self).__init__()\n",
        "\n",
        "        self.layers_size = layers_size\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_embedding = PositionalEmbedding(max_length_en, v_size_en, layers_size)\n",
        "        self.enc_layers = [EncoderLayer(layers_size, dense_size, num_heads) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, mask=None) -> tf.Tensor:\n",
        "        # src_embeddings = PositionalEmbedding(max_length_en, v_size_en, layers_size)(src_embeddings)\n",
        "        src_embeddings = self.pos_embedding(inputs)\n",
        "        enc_out = self.dropout(src_embeddings)\n",
        "        for i in range(self.num_layers):\n",
        "            enc_out = self.enc_layers[i](enc_out)\n",
        "\n",
        "        return enc_out  # (batch_size, input_seq_len, layers_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk5x1zELSFyl"
      },
      "source": [
        "###Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WwlG430kpb7"
      },
      "source": [
        "The single layer of the decoder transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVmDmBHsSLHo"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, layers_size: int, dense_size: int, num_heads: int, **kwargs) -> None:\n",
        "        super(DecoderLayer, self).__init__(**kwargs)\n",
        "        \n",
        "        self.layers_size = layers_size\n",
        "        self.dense_size = dense_size\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads, layers_size)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads, layers_size)\n",
        "        self.dense_proj = tf.keras.Sequential(\n",
        "            [layers.Dense(dense_size, activation=\"relu\"), layers.Dense(layers_size)]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, encoder_outputs: tf.Tensor, mask=None) -> tf.Tensor:\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFQJbb3nSwhV"
      },
      "source": [
        "class DecoderTransformer(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 layers_size: int,\n",
        "                 dense_size: int,\n",
        "                 num_heads: int,\n",
        "                 dropout=0.1) -> None:\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.layers_size = layers_size\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_embedding = PositionalEmbedding(max_length_it, v_size_it, layers_size)\n",
        "        self.dec_layers = [DecoderLayer(layers_size, dense_size, num_heads) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, enc_output: tf.Tensor, mask=None) -> tf.Tensor:\n",
        "        dst_embeddings = self.pos_embedding(inputs)\n",
        "        dec_output = self.dropout(dst_embeddings)\n",
        "        for i in range(self.num_layers):\n",
        "            dec_output = self.dec_layers[i](dec_output, enc_output)\n",
        "\n",
        "        return dec_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9z1wDY_JW43"
      },
      "source": [
        "##Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omRwwkoqJd6O"
      },
      "source": [
        "layers_size = 512\n",
        "num_layers = 6\n",
        "dense_size = 2048\n",
        "num_heads = 8\n",
        "\n",
        "def create_model():\n",
        "    # Encoder\n",
        "    encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    encoder_outputs = EncoderTransformer(num_layers, layers_size, dense_size, num_heads)(encoder_inputs)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = tf.keras.Input(shape=(None, layers_size), name=\"decoder_state_inputs\")\n",
        "    decoder_outputs = DecoderTransformer(num_layers, layers_size, dense_size, num_heads)(decoder_inputs, encoded_seq_inputs)\n",
        "    decoder_outputs = layers.Dropout(0.4)(decoder_outputs)\n",
        "    decoder_outputs = layers.Dense(v_size_it, activation=\"softmax\")(decoder_outputs)\n",
        "    decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"decoder_transformer\")\n",
        "    # decoder.summary()\n",
        "\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
        "    # transformer.summary()\n",
        "    return transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBji4C_qmXg-",
        "outputId": "459ae280-6279-4764-d2c8-411d2ea0a3d6"
      },
      "source": [
        "with strategy.scope():\n",
        "    opt = tf.keras.optimizers.Nadam(learning_rate = 0.0001)\n",
        "    transformer = create_model()\n",
        "    transformer.summary()\n",
        "    transformer.compile(opt, loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_transformer_2 (EncoderT (None, None, 256)    17282816    encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_transformer (Functional (None, None, 31102)  31736702    decoder_inputs[0][0]             \n",
            "                                                                 encoder_transformer_2[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 49,019,518\n",
            "Trainable params: 49,019,518\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccFwVc9HU70h"
      },
      "source": [
        "##Train and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ3VnYtqU9KQ",
        "outputId": "0e1bda5c-79ec-4c2c-a637-efbc908846ad"
      },
      "source": [
        "transformer.fit(tr_batches, epochs=20, validation_data = val_batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 11) dtype=int32>]\n",
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 11) dtype=int32>]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2194/2194 [==============================] - ETA: 0s - loss: 2.7322 - accuracy: 0.5120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 11) dtype=int32>]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2194/2194 [==============================] - 208s 64ms/step - loss: 2.7322 - accuracy: 0.5120 - val_loss: 1.6538 - val_accuracy: 0.6338\n",
            "Epoch 2/10\n",
            "2194/2194 [==============================] - 99s 45ms/step - loss: 1.3916 - accuracy: 0.6839 - val_loss: 1.0321 - val_accuracy: 0.7474\n",
            "Epoch 3/10\n",
            "2194/2194 [==============================] - 99s 45ms/step - loss: 0.9543 - accuracy: 0.7594 - val_loss: 0.7287 - val_accuracy: 0.8004\n",
            "Epoch 4/10\n",
            "2194/2194 [==============================] - 98s 45ms/step - loss: 0.7230 - accuracy: 0.7984 - val_loss: 0.5629 - val_accuracy: 0.8290\n",
            "Epoch 5/10\n",
            "2194/2194 [==============================] - 100s 45ms/step - loss: 0.5855 - accuracy: 0.8218 - val_loss: 0.4629 - val_accuracy: 0.8469\n",
            "Epoch 6/10\n",
            "2194/2194 [==============================] - 100s 45ms/step - loss: 0.4962 - accuracy: 0.8372 - val_loss: 0.3961 - val_accuracy: 0.8598\n",
            "Epoch 7/10\n",
            "2194/2194 [==============================] - 100s 46ms/step - loss: 0.4350 - accuracy: 0.8486 - val_loss: 0.3518 - val_accuracy: 0.8682\n",
            "Epoch 8/10\n",
            "2194/2194 [==============================] - 99s 45ms/step - loss: 0.3891 - accuracy: 0.8573 - val_loss: 0.3206 - val_accuracy: 0.8744\n",
            "Epoch 9/10\n",
            "2194/2194 [==============================] - 100s 45ms/step - loss: 0.3548 - accuracy: 0.8637 - val_loss: 0.2981 - val_accuracy: 0.8791\n",
            "Epoch 10/10\n",
            "2194/2194 [==============================] - 100s 45ms/step - loss: 0.3277 - accuracy: 0.8692 - val_loss: 0.2797 - val_accuracy: 0.8826\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff9b921a090>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOvr3NkVg1Pi",
        "outputId": "666f27dd-8833-4513-c7de-626211f66aac"
      },
      "source": [
        "ts_loss, ts_accuracy = transformer.evaluate(make_batches(ts_set, 128))\n",
        "print(\"Test loss: {0}\\nTest accuracy: {1}\".format(ts_loss, ts_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "275/275 [==============================] - 7s 18ms/step - loss: 0.2828 - accuracy: 0.8813\n",
            "Test loss: 0.2828167974948883\n",
            "Test accuracy: 0.8812545537948608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM1rCjwLZii0"
      },
      "source": [
        "##Translator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPl3JEhZZn9u"
      },
      "source": [
        "max_decoded_sentence_length = 10\n",
        "\n",
        "def translate(input_sentence):\n",
        "    tokenized_input_sentence = tokenizer_en(input_sentence, return_tensors='tf', add_special_tokens=True, max_length = max_length_en, padding='max_length').data[\"input_ids\"]\n",
        "    decoded_sentence = \"[CLS]\"\n",
        "    list_tokens=[]\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = tokenizer_it(decoded_sentence, return_tensors='tf', add_special_tokens=False, max_length = max_length_it, padding='max_length').data['input_ids']\n",
        "        predictions = transformer.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = tokenizer_it.ids_to_tokens[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[SEP]\":\n",
        "            break\n",
        "        \n",
        "        list_tokens.append(sampled_token) \n",
        "      \n",
        "    return decoded_sentence, tokenizer_it.convert_tokens_to_string(list_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oOunG1G8aWGP",
        "outputId": "56984ca7-0adb-4c1e-9122-8e93b689a41b"
      },
      "source": [
        "tokens , translated = translate(\"This task is very hard.\")\n",
        "print(translated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 11) dtype=int32>]\n",
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 11) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 13) dtype=int32>]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-e1e1512e4d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This task is very hard.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-7958eb4c78d3>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_decoded_sentence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtokenized_target_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_input_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_target_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0msampled_token_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msampled_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_it\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids_to_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3459\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3460\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3382\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    <ipython-input-20-acdba27c0170>:22 call  *\n        dec_output = self.dec_layers[i](dec_output, enc_output)\n    <ipython-input-19-f79578c31219>:29 call  *\n        attention_output_2 = self.attention_2(\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py:504 call\n        query, key, value, attention_mask, training)\n    /usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py:468 _compute_attention\n        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n    /usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py:431 _masked_softmax\n        return self._softmax(attention_scores, attention_mask)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/layers/advanced_activations.py:341 call\n        inputs += adder\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py:465 add_v2\n        \"AddV2\", x=x, y=y, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:3569 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:2042 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 11 and 13 for '{{node transformer/decoder_transformer/decoder_transformer_2/decoder_layer_6/multi_head_attention_22/softmax_1/add}} = AddV2[T=DT_FLOAT](transformer/decoder_transformer/decoder_transformer_2/decoder_layer_6/multi_head_attention_22/einsum/Einsum, transformer/decoder_transformer/decoder_transformer_2/decoder_layer_6/multi_head_attention_22/softmax_1/mul)' with input shapes: [?,8,13,11], [?,1,?,13].\n"
          ]
        }
      ]
    }
  ]
}